from text.tokenizer import make_tokenized
from text.makedictionary import make_word_count, make_word2index
import math
from collections import defaultdict
from tqdm import tqdm

import data


class NaiveBayesClassifier():

    '''
    Task : íŠ¹ì • ë¬¸ì„œê°€ ë“¤ì–´ì™”ì„ ë•Œ ì´ë¥¼ ì ì ˆí•œ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•˜ëŠ” ëª¨ë¸
    * í´ë˜ìŠ¤ê°€ CV, NLP ë‘ ê°œê°€ ì£¼ì–´ì§ˆ ë•Œ íŠ¹ì • ë¬¸ì„œê°€ ë“¤ì–´ì˜¤ë©´ ë‘˜ ì¤‘ ì ì ˆí•œ í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜í•œë‹¤.
    * ì´ê³³ì—ì„œëŠ” 0,1 ì´ì§„ ë¶„ë¥˜ì˜ ê²½ìš°ë§Œ êµ¬í˜„ë˜ì–´ìˆì–´ í´ë˜ìŠ¤ê°€ 3ê°œ ì´ìƒì¸ ê²½ìš° ì¶”ê°€ êµ¬í˜„í•´ì•¼í•œë‹¤.


    Task ì„¤ëª… : argmax of c ğ‘ƒ(c|d)ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ê³  ì´ëŠ” ë² ì´ì¦ˆë£°ì— ì˜í•´ ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•´ì§„ë‹¤. 
               (ğ‘ƒ(c|d) : ë¬¸ì„œê°€ íŠ¹ì • í´ë˜ìŠ¤ë¡œ ë¶„ë¥˜ë  í™•ë¥ )
               = argmax of c ğ‘ƒ(ğ‘‘|ğ‘)*P(ğ‘) = argmax of c ğ‘ƒ(ğ‘¤1, ğ‘¤2, . . . , ğ‘¤n|ğ‘)P(ğ‘) â†’ P(ğ‘) âˆwiâˆˆW ğ‘ƒ(wi|ğ‘)
               (wi : ë¬¸ì„œì— í¬í•¨ëœ ië²ˆ ì§¸ ë‹¨ì–´)

    __init__
        self.k: Smoothingì„ ìœ„í•œ ìƒìˆ˜.
        self.w2i: ì‚¬ì „ì— êµ¬í•œ vocab.
        self.priors: ê° classì˜ prior í™•ë¥ (: P(c)) -> classi : P(classi) ë”•ì…”ë„ˆë¦¬
        self.likelihoods: ê° tokenì˜ íŠ¹ì • class ì¡°ê±´ ë‚´ì—ì„œì˜ likelihood. (: ğ‘ƒ(wi|ğ‘)) 
                        -> tokeni : {class1: ğ‘ƒ(tokeni|ğ‘lassi), ... , classN: ğ‘ƒ(tokenN|ğ‘lassN)} ë”•ì…”ë„ˆë¦¬

    train
        1. set_priors : ì‚¬ì „í™•ë¥ (: P(c)) êµ¬í•˜ê¸° = c ë¬¸ì„œì˜ ê°¯ìˆ˜ / ì „ì²´ë¬¸ì„œì˜ ê°¯ìˆ˜
        2. set_likelihoods : ì „ì²´ ë¬¸ì„œì— ë“±ì¥í•˜ëŠ” ê° ë‹¨ì–´ì— ëŒ€í•œ likelihood(: ğ‘ƒ(wj|ğ‘)) êµ¬í•˜ê¸° 
                            = cí´ë˜ìŠ¤ì— ë“±ì¥í•˜ëŠ” wj ê°¯ìˆ˜ / cí´ë˜ìŠ¤ì— ë“±ì¥í•˜ëŠ” ì „ì²´ ë‹¨ì–´ ê°¯ìˆ˜ 

    inference
        tokens(token1, ... tokenN)ê°€ ì£¼ì–´ì§ˆ ë•Œ argmax of c P(ğ‘) âˆwiâˆˆW ğ‘ƒ(wi|ğ‘) êµ¬í•˜ì—¬ cê°’ ë°˜í™˜
            
    '''

    def __init__(self, w2i, k=0.1):
        self.k = k
        self.w2i = w2i
        self.priors = {}
        self.likelihoods = {}

    def train(self, train_tokenized, train_labels):
        self.set_priors(train_labels)  # Priors ê³„ì‚°.
        self.set_likelihoods(train_tokenized, train_labels)  # Likelihoods ê³„ì‚°.

    def inference(self, tokens):
        log_prob0 = 0.0
        log_prob1 = 0.0

        for token in tokens:
            if token in self.likelihoods:  # í•™ìŠµ ë‹¹ì‹œ ì¶”ê°€í–ˆë˜ ë‹¨ì–´ì— ëŒ€í•´ì„œë§Œ ê³ ë ¤.
                log_prob0 += math.log(self.likelihoods[token][0])
                log_prob1 += math.log(self.likelihoods[token][1])

        # ë§ˆì§€ë§‰ì— priorë¥¼ ê³ ë ¤.
        log_prob0 += math.log(self.priors[0])
        log_prob1 += math.log(self.priors[1])

        if log_prob0 >= log_prob1:
            return 0
        else:
            return 1

    def set_priors(self, train_labels):
        class_counts = defaultdict(int)
        for label in tqdm(train_labels):
            class_counts[label] += 1

        for label, count in class_counts.items():
            self.priors[label] = class_counts[label] / len(train_labels)

    def set_likelihoods(self, train_tokenized, train_labels):
        token_dists = {}  # ê° ë‹¨ì–´ì˜ íŠ¹ì • class ì¡°ê±´ í•˜ì—ì„œì˜ ë“±ì¥ íšŸìˆ˜.
        class_counts = defaultdict(int)  # íŠ¹ì • classì—ì„œ ë“±ì¥í•œ ëª¨ë“  ë‹¨ì–´ì˜ ë“±ì¥ íšŸìˆ˜.

        for i, label in enumerate(tqdm(train_labels)):
            count = 0
            for token in train_tokenized[i]:
                if token in self.w2i:  # í•™ìŠµ ë°ì´í„°ë¡œ êµ¬ì¶•í•œ vocabì— ìˆëŠ” tokenë§Œ ê³ ë ¤.
                    if token not in token_dists:
                        token_dists[token] = {0:0, 1:0}
                        token_dists[token][label] += 1
                        count += 1
                class_counts[label] += count

        for token, dist in tqdm(token_dists.items()):
            if token not in self.likelihoods:
                self.likelihoods[token] = {
                    0:(token_dists[token][0] + self.k) / (class_counts[0] + len(self.w2i)*self.k),
                    1:(token_dists[token][1] + self.k) / (class_counts[1] + len(self.w2i)*self.k),
                }


if __name__ =='__main__':

    train_data = data.train_data
    train_labels = data.train_labels
    test_data = data.test_data

    # tokenize
    train_tokenized = make_tokenized(train_data)
    test_tokenized = make_tokenized(test_data)


    # print(train_tokenized)
    # print(test_tokenized)


    # ì‚¬ì „ ë§Œë“¤ê¸°
    word_count = make_word_count(train_tokenized)
    w2i = make_word2index(word_count)

    # print(word_count)
    # print(w2i)


    # ëª¨ë¸ í›ˆë ¨í•˜ê¸°
    classifier = NaiveBayesClassifier(w2i)
    classifier.train(train_tokenized, train_labels)


    # ëª¨ë¸ í…ŒìŠ¤íŠ¸í•˜ê¸°
    preds = []
    for test_tokens in tqdm(test_tokenized):
        pred = classifier.inference(test_tokens)
        preds.append(pred)
    print(preds)